{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "sweater_style.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXx1RrTAEUxn",
        "colab_type": "code",
        "colab": {},
        "outputId": "289d4d68-4dea-4122-ac94-8dab924d6808"
      },
      "source": [
        "!pip install git+https://github.com/kkrunal77/data_science_utils.git#egg=data_science_utils"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting data_science_utils from git+https://github.com/kkrunal77/data_science_utils.git#egg=data_science_utils\n",
            "  Cloning https://github.com/kkrunal77/data_science_utils.git to /tmp/pip-build-bRqUuy/data-science-utils\n",
            "Collecting numpy (from data_science_utils)\n",
            "  Using cached https://files.pythonhosted.org/packages/cb/79/96df883cd6df0c86cb010e6f4ff790b7a30a45016a9509c94ea72c8695cd/numpy-1.17.1.zip\n",
            "    Complete output from command python setup.py egg_info:\n",
            "    Traceback (most recent call last):\n",
            "      File \"<string>\", line 1, in <module>\n",
            "      File \"/tmp/pip-build-bRqUuy/numpy/setup.py\", line 31, in <module>\n",
            "        raise RuntimeError(\"Python version >= 3.5 required.\")\n",
            "    RuntimeError: Python version >= 3.5 required.\n",
            "    \n",
            "    ----------------------------------------\n",
            "\u001b[31mCommand \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-build-bRqUuy/numpy/\u001b[0m\n",
            "\u001b[33mYou are using pip version 8.1.1, however version 19.2.3 is available.\n",
            "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ar7X4p6sEUxy",
        "colab_type": "code",
        "colab": {},
        "outputId": "fc9f40b1-31e8-4a0a-907f-a17f699ce3f1"
      },
      "source": [
        "import imports"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'imports'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-0ca35a79fcb9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mimports\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'imports'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9ks5vHAEUx_",
        "colab_type": "code",
        "colab": {},
        "outputId": "5cfaeb8d-db84-44ef-b7f9-77698d229d1b"
      },
      "source": [
        "import keras\n",
        "print(keras.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycFRrJyrEUyG",
        "colab_type": "code",
        "colab": {},
        "outputId": "b0778a6d-ba1f-4e25-ce40-03be89de37c1"
      },
      "source": [
        "from tensorflow import keras\n",
        "print(keras.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.1.6-tf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_Lm__zPEUyN",
        "colab_type": "code",
        "colab": {},
        "outputId": "ccabbd0e-75c4-4bc4-e492-500d8d96dadd"
      },
      "source": [
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.utils import multi_gpu_model\n",
        "from tensorflow.keras.applications import xception,InceptionV3#change the architecture\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications.xception import preprocess_input\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.optimizers import Nadam, SGD\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler,ModelCheckpoint,TensorBoard,EarlyStopping\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "# import Callback\n",
        "\n",
        "\"\"\"config the model input dim, number of labels to classify, batch size to be processed\"\"\"\n",
        "\n",
        "img_width,img_height = 299,299\n",
        "\n",
        "classes = 4\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# train_data_dir = '/home/hitagi/uni_pat_new/DATA/train'\n",
        "\n",
        "# validation_data_dir = '/home/hitagi/uni_pat_new/DATA/val'\n",
        "\n",
        "train_data_dir = '/home/eleven/krunal/kk/sweater_style/train'\n",
        "validation_data_dir = '/home/eleven/krunal/kk/sweater_style/test'\n",
        "\n",
        "txt_file = \"sweater_style.txt\"\n",
        "\n",
        "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input, rotation_range=0.3, shear_range=10, zoom_range=0.1, cval=0.0, horizontal_flip=True, vertical_flip=True)\n",
        "validation_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "train_generator = train_datagen.flow_from_directory(train_data_dir, target_size=(img_width, img_height), batch_size=batch_size, class_mode='categorical')\n",
        "validation_generator = validation_datagen.flow_from_directory(validation_data_dir, target_size=(img_width, img_height), batch_size=batch_size,class_mode='categorical')\n",
        "\n",
        "print(train_generator.class_indices)\n",
        "dict_que = train_generator.class_indices\n",
        "\n",
        "mapped_que = {}\n",
        "for key,val in zip(dict_que.keys(),dict_que.values()):\n",
        "        mapped_que[val] = key\n",
        "        \n",
        "text = open(txt_file,\"w\")\n",
        "text.write(str(mapped_que))\n",
        "text.close()\n",
        "\n",
        "\n",
        "def know_samples():\n",
        "    train_total_batches = train_generator.__len__()\n",
        "    train_total_samples =  train_generator.samples\n",
        "    val_total_batches = validation_generator.__len__()\n",
        "    val_total_samples = validation_generator.samples\n",
        "    train_b_size = train_total_samples/train_total_batches\n",
        "    val_b_size = val_total_samples/val_total_batches\n",
        "    return (\"(TRAIN)a batch is produced with {} images\".format(int(np.ceil(train_b_size)))),(\"(TRAIN)total batches are {} in count\".format(train_total_batches),(\"(VAL)a batch is produced with {} images\".format(int(np.ceil(val_b_size)))),(\"(VAL)total batches are {} in count\".format(val_total_batches)))\n",
        "\n",
        "\n",
        "def step_decay_schedule(initial_lr=None, decay_factor=None, step_size=None):\n",
        "    def schedule(epoch):\n",
        "        return initial_lr * (decay_factor ** np.floor(epoch/step_size))\n",
        "    \n",
        "    return LearningRateScheduler(schedule)\n",
        "\n",
        "know_samples()\n",
        "\n",
        "# final_xception224.h5\n",
        "\n",
        "top_weights_path = 'sweater_style.h5'\n",
        "final_weights_path = 'final_sweater_style.h5'\n",
        "\n",
        "os.mkdir(\"./logs_sweater_style_3\")\n",
        "tensorboard_dir = TensorBoard(log_dir=\"logs/\",write_images=True)\n",
        "base_model = xception.Xception(input_shape=(img_width, img_height, 3), weights='imagenet', include_top=False)\n",
        "# base_model = InceptionV3(input_shape=(img_width, img_height, 3), weights='imagenet', include_top=False)\n",
        "\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "predictions = Dense(classes, activation='softmax')(x)\n",
        "model = Model(base_model.input, predictions)\n",
        "\n",
        "train_steps = train_generator.__len__()\n",
        "val_steps = validation_generator.__len__()\n",
        "\n",
        "\n",
        "print(\"training 1st stage\")\n",
        "\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable =  True\n",
        "\n",
        "\n",
        "    \n",
        "model = multi_gpu_model(model,2)\n",
        "model.compile(optimizer=\"Nadam\",loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "lr_sched = step_decay_schedule(initial_lr=1e-5, decay_factor=0.75, step_size=100)\n",
        "\n",
        "callbacks_list = [\n",
        "    ModelCheckpoint(top_weights_path, monitor='val_acc', verbose=1, save_best_only=True),\n",
        "    EarlyStopping(monitor='val_acc', patience=2, verbose=0),lr_sched\n",
        "]\n",
        "\n",
        "history = model.fit_generator(train_generator, steps_per_epoch=train_steps, epochs = 18, validation_data=validation_generator, validation_steps=val_steps, callbacks=callbacks_list,verbose=1,workers=8,max_queue_size=train_steps*2)\n",
        "print(\"training 2nd stage\")\n",
        "\n",
        "model.load_weights(top_weights_path)\n",
        "for layer in model.layers[:155]:\n",
        "    layer.trainable = False\n",
        "    \n",
        "for layer in model.layers[155:]:\n",
        "    layer.trainable = True\n",
        "    \n",
        "model.compile(optimizer=\"Nadam\",loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "lr_sched = step_decay_schedule(initial_lr=1e-7, decay_factor=0.55, step_size=100)\n",
        "\n",
        "callbacks_list = [\n",
        "    ModelCheckpoint(final_weights_path, monitor='val_acc', verbose=1, save_best_only=True),\n",
        "    EarlyStopping(monitor='val_acc', patience=2, verbose=0),lr_sched\n",
        "    ]\n",
        "history = model.fit_generator(train_generator, steps_per_epoch=train_steps, epochs = 10/1, validation_data=validation_generator, validation_steps=val_steps, callbacks=callbacks_list,verbose=1,workers=8,max_queue_size=train_steps*2)\n",
        "print(\"training 3nd stage\")\n",
        "\n",
        "model.load_weights(final_weights_path)\n",
        "for layer in model.layers[:165]:\n",
        "    layer.trainable = False\n",
        "    \n",
        "for layer in model.layers[165:]:\n",
        "    layer.trainable = True\n",
        "    \n",
        "model.compile(optimizer=\"Nadam\",loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "lr_sched = step_decay_schedule(initial_lr=1e-7, decay_factor=0.55, step_size=100)\n",
        "\n",
        "callbacks_list = [\n",
        "    ModelCheckpoint(final_weights_path, monitor='val_acc', verbose=1, save_best_only=True),\n",
        "    EarlyStopping(monitor='val_acc', patience=2, verbose=0),lr_sched\n",
        "    ]\n",
        "history = model.fit_generator(train_generator, steps_per_epoch=train_steps, epochs = 10/1, validation_data=validation_generator, validation_steps=val_steps, callbacks=callbacks_list,verbose=1,workers=8,max_queue_size=train_steps*2)\n",
        "print(\"ensembled_module retrain\")\n",
        "\n",
        "model.load_weights(final_weights_path)\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "    \n",
        "model.compile(optimizer=\"Nadam\",loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "lr_sched = step_decay_schedule(initial_lr=1e-7, decay_factor=0.65, step_size=100)\n",
        "\n",
        "callbacks_list = [\n",
        "    ModelCheckpoint(final_weights_path, monitor='val_acc', verbose=1, save_best_only=True),\n",
        "    EarlyStopping(monitor='val_acc', patience=2, verbose=0),lr_sched,tensorboard_dir\n",
        "    ]\n",
        "\n",
        "history = model.fit_generator(train_generator, steps_per_epoch=train_steps, epochs = 6/1, validation_data=validation_generator, validation_steps=val_steps, callbacks=callbacks_list,verbose=1,workers=8,max_queue_size=train_steps*2)\n",
        "\n",
        "\n",
        "print(\"-----Completed------\")\n",
        "print(\"Note: use {} for prediction:\".format(final_weights_path))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1490 images belonging to 4 classes.\n",
            "Found 400 images belonging to 4 classes.\n",
            "{'Cardigan': 0, 'Poncho': 1, 'Pullover': 2, 'Sweater_Vest': 3}\n",
            "training 1st stage\n",
            "Epoch 1/18\n",
            "46/47 [============================>.] - ETA: 1s - loss: 0.5537 - acc: 0.7497\n",
            "Epoch 00001: val_acc improved from -inf to 0.75125, saving model to sweater_style.h5\n",
            "47/47 [==============================] - 54s 1s/step - loss: 0.5530 - acc: 0.7497 - val_loss: 0.4990 - val_acc: 0.7512\n",
            "Epoch 2/18\n",
            "46/47 [============================>.] - ETA: 0s - loss: 0.4855 - acc: 0.7600\n",
            "Epoch 00002: val_acc improved from 0.75125 to 0.78938, saving model to sweater_style.h5\n",
            "47/47 [==============================] - 31s 654ms/step - loss: 0.4852 - acc: 0.7601 - val_loss: 0.4208 - val_acc: 0.7894\n",
            "Epoch 3/18\n",
            "46/47 [============================>.] - ETA: 0s - loss: 0.4065 - acc: 0.8008\n",
            "Epoch 00003: val_acc improved from 0.78938 to 0.86000, saving model to sweater_style.h5\n",
            "47/47 [==============================] - 31s 654ms/step - loss: 0.4056 - acc: 0.8018 - val_loss: 0.3375 - val_acc: 0.8600\n",
            "Epoch 4/18\n",
            "46/47 [============================>.] - ETA: 0s - loss: 0.3378 - acc: 0.8618\n",
            "Epoch 00004: val_acc improved from 0.86000 to 0.91750, saving model to sweater_style.h5\n",
            "47/47 [==============================] - 31s 659ms/step - loss: 0.3368 - acc: 0.8624 - val_loss: 0.2600 - val_acc: 0.9175\n",
            "Epoch 5/18\n",
            "46/47 [============================>.] - ETA: 0s - loss: 0.2755 - acc: 0.9084\n",
            "Epoch 00005: val_acc improved from 0.91750 to 0.93875, saving model to sweater_style.h5\n",
            "47/47 [==============================] - 31s 650ms/step - loss: 0.2744 - acc: 0.9090 - val_loss: 0.2002 - val_acc: 0.9387\n",
            "Epoch 6/18\n",
            "46/47 [============================>.] - ETA: 0s - loss: 0.2196 - acc: 0.9326\n",
            "Epoch 00006: val_acc improved from 0.93875 to 0.95063, saving model to sweater_style.h5\n",
            "47/47 [==============================] - 31s 663ms/step - loss: 0.2213 - acc: 0.9314 - val_loss: 0.1639 - val_acc: 0.9506\n",
            "Epoch 7/18\n",
            "46/47 [============================>.] - ETA: 0s - loss: 0.1805 - acc: 0.9433\n",
            "Epoch 00007: val_acc improved from 0.95063 to 0.95562, saving model to sweater_style.h5\n",
            "47/47 [==============================] - 31s 662ms/step - loss: 0.1825 - acc: 0.9424 - val_loss: 0.1402 - val_acc: 0.9556\n",
            "Epoch 8/18\n",
            "46/47 [============================>.] - ETA: 0s - loss: 0.1576 - acc: 0.9499\n",
            "Epoch 00008: val_acc improved from 0.95562 to 0.95937, saving model to sweater_style.h5\n",
            "47/47 [==============================] - 31s 656ms/step - loss: 0.1577 - acc: 0.9500 - val_loss: 0.1252 - val_acc: 0.9594\n",
            "Epoch 9/18\n",
            "46/47 [============================>.] - ETA: 0s - loss: 0.1359 - acc: 0.9555\n",
            "Epoch 00009: val_acc improved from 0.95937 to 0.96188, saving model to sweater_style.h5\n",
            "47/47 [==============================] - 31s 658ms/step - loss: 0.1350 - acc: 0.9560 - val_loss: 0.1153 - val_acc: 0.9619\n",
            "Epoch 10/18\n",
            "46/47 [============================>.] - ETA: 0s - loss: 0.1202 - acc: 0.9625\n",
            "Epoch 00010: val_acc improved from 0.96188 to 0.96437, saving model to sweater_style.h5\n",
            "47/47 [==============================] - 31s 660ms/step - loss: 0.1207 - acc: 0.9625 - val_loss: 0.1087 - val_acc: 0.9644\n",
            "Epoch 11/18\n",
            "46/47 [============================>.] - ETA: 0s - loss: 0.1002 - acc: 0.9689\n",
            "Epoch 00011: val_acc did not improve from 0.96437\n",
            "47/47 [==============================] - 30s 634ms/step - loss: 0.1003 - acc: 0.9689 - val_loss: 0.1045 - val_acc: 0.9631\n",
            "Epoch 12/18\n",
            "46/47 [============================>.] - ETA: 0s - loss: 0.0962 - acc: 0.9716\n",
            "Epoch 00012: val_acc improved from 0.96437 to 0.96625, saving model to sweater_style.h5\n",
            "47/47 [==============================] - 31s 656ms/step - loss: 0.0954 - acc: 0.9721 - val_loss: 0.1017 - val_acc: 0.9663\n",
            "Epoch 13/18\n",
            "46/47 [============================>.] - ETA: 0s - loss: 0.0871 - acc: 0.9708\n",
            "Epoch 00013: val_acc improved from 0.96625 to 0.96813, saving model to sweater_style.h5\n",
            "47/47 [==============================] - 31s 657ms/step - loss: 0.0872 - acc: 0.9710 - val_loss: 0.0978 - val_acc: 0.9681\n",
            "Epoch 14/18\n",
            "46/47 [============================>.] - ETA: 0s - loss: 0.0832 - acc: 0.9728\n",
            "Epoch 00014: val_acc did not improve from 0.96813\n",
            "47/47 [==============================] - 30s 631ms/step - loss: 0.0835 - acc: 0.9727 - val_loss: 0.0951 - val_acc: 0.9681\n",
            "Epoch 15/18\n",
            "46/47 [============================>.] - ETA: 0s - loss: 0.0790 - acc: 0.9754\n",
            "Epoch 00015: val_acc improved from 0.96813 to 0.96937, saving model to sweater_style.h5\n",
            "47/47 [==============================] - 31s 656ms/step - loss: 0.0787 - acc: 0.9757 - val_loss: 0.0947 - val_acc: 0.9694\n",
            "Epoch 16/18\n",
            "46/47 [============================>.] - ETA: 0s - loss: 0.0606 - acc: 0.9839\n",
            "Epoch 00016: val_acc did not improve from 0.96937\n",
            "47/47 [==============================] - 30s 637ms/step - loss: 0.0620 - acc: 0.9827 - val_loss: 0.0926 - val_acc: 0.9694\n",
            "Epoch 17/18\n",
            "46/47 [============================>.] - ETA: 0s - loss: 0.0616 - acc: 0.9820\n",
            "Epoch 00017: val_acc did not improve from 0.96937\n",
            "47/47 [==============================] - 30s 637ms/step - loss: 0.0621 - acc: 0.9814 - val_loss: 0.0928 - val_acc: 0.9688\n",
            "training 2nd stage\n",
            "Epoch 1/10\n",
            "46/47 [============================>.] - ETA: 0s - loss: 0.0656 - acc: 0.9816\n",
            "Epoch 00001: val_acc improved from -inf to 0.96937, saving model to final_sweater_style.h5\n",
            "47/47 [==============================] - 20s 432ms/step - loss: 0.0659 - acc: 0.9813 - val_loss: 0.0938 - val_acc: 0.9694\n",
            "Epoch 2/10\n",
            "46/47 [============================>.] - ETA: 0s - loss: 0.0632 - acc: 0.9822\n",
            "Epoch 00002: val_acc did not improve from 0.96937\n",
            "47/47 [==============================] - 15s 322ms/step - loss: 0.0638 - acc: 0.9823 - val_loss: 0.0951 - val_acc: 0.9681\n",
            "Epoch 3/10\n",
            "46/47 [============================>.] - ETA: 0s - loss: 0.0778 - acc: 0.9742\n",
            "Epoch 00003: val_acc did not improve from 0.96937\n",
            "47/47 [==============================] - 15s 327ms/step - loss: 0.0769 - acc: 0.9745 - val_loss: 0.0941 - val_acc: 0.9688\n",
            "training 3nd stage\n",
            "Epoch 1/10\n",
            "46/47 [============================>.] - ETA: 0s - loss: 0.0761 - acc: 0.9789\n",
            "Epoch 00001: val_acc improved from -inf to 0.96937, saving model to final_sweater_style.h5\n",
            "47/47 [==============================] - 20s 423ms/step - loss: 0.0754 - acc: 0.9792 - val_loss: 0.0947 - val_acc: 0.9694\n",
            "Epoch 2/10\n",
            "46/47 [============================>.] - ETA: 0s - loss: 0.0670 - acc: 0.9774\n",
            "Epoch 00002: val_acc did not improve from 0.96937\n",
            "47/47 [==============================] - 15s 324ms/step - loss: 0.0668 - acc: 0.9776 - val_loss: 0.0938 - val_acc: 0.9694\n",
            "Epoch 3/10\n",
            "46/47 [============================>.] - ETA: 0s - loss: 0.0721 - acc: 0.9796\n",
            "Epoch 00003: val_acc improved from 0.96937 to 0.97000, saving model to final_sweater_style.h5\n",
            "47/47 [==============================] - 15s 330ms/step - loss: 0.0724 - acc: 0.9794 - val_loss: 0.0938 - val_acc: 0.9700\n",
            "Epoch 4/10\n",
            "46/47 [============================>.] - ETA: 0s - loss: 0.0633 - acc: 0.9805\n",
            "Epoch 00004: val_acc did not improve from 0.97000\n",
            "47/47 [==============================] - 15s 325ms/step - loss: 0.0625 - acc: 0.9809 - val_loss: 0.0938 - val_acc: 0.9700\n",
            "Epoch 5/10\n",
            "46/47 [============================>.] - ETA: 0s - loss: 0.0728 - acc: 0.9742\n",
            "Epoch 00005: val_acc did not improve from 0.97000\n",
            "47/47 [==============================] - 15s 329ms/step - loss: 0.0734 - acc: 0.9739 - val_loss: 0.0944 - val_acc: 0.9700\n",
            "ensembled_module retrain\n",
            "Epoch 1/6\n",
            "46/47 [============================>.] - ETA: 0s - loss: 0.0670 - acc: 0.9830\n",
            "Epoch 00001: val_acc improved from -inf to 0.97000, saving model to final_sweater_style.h5\n",
            "47/47 [==============================] - 15s 324ms/step - loss: 0.0662 - acc: 0.9834 - val_loss: 0.0938 - val_acc: 0.9700\n",
            "Epoch 2/6\n",
            "46/47 [============================>.] - ETA: 0s - loss: 0.0704 - acc: 0.9796\n",
            "Epoch 00002: val_acc did not improve from 0.97000\n",
            "47/47 [==============================] - 11s 225ms/step - loss: 0.0707 - acc: 0.9789 - val_loss: 0.0938 - val_acc: 0.9700\n",
            "Epoch 3/6\n",
            "46/47 [============================>.] - ETA: 0s - loss: 0.0676 - acc: 0.9795\n",
            "Epoch 00003: val_acc did not improve from 0.97000\n",
            "47/47 [==============================] - 13s 273ms/step - loss: 0.0670 - acc: 0.9800 - val_loss: 0.0938 - val_acc: 0.9700\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "-----Completed------\n",
            "Note: use final_sweater_style.h5 for prediction:\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfcGqm-iEUyT",
        "colab_type": "code",
        "colab": {},
        "outputId": "bd7e0efe-e6a1-4918-84b2-2f36ad0e0590"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.plot(history.history['loss'])\n",
        "plt.title('loss')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.plot(history.history['acc'])\n",
        "plt.title('acc')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxhSWtEVEUyY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_path = '/home/eleven/kk/sweater_style'\n",
        "model.load_weights(final_weights_path)\n",
        "model.save(os.path.join(os.path.abspath(model_path), 'final_sweater_style.h5'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUwTFawmEUyd",
        "colab_type": "code",
        "colab": {},
        "outputId": "350c72c6-1f03-450c-e597-795c0e59bfe7"
      },
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "model = load_model('final_sweater_style.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7vaxoFIEUyi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label_dict = {j:i for i,j in train_generator.class_indices.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fJfcNIUEUym",
        "colab_type": "code",
        "colab": {},
        "outputId": "2872ec56-0c97-48d6-f741-bc2703075d18"
      },
      "source": [
        "label_dict"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'Cardigan', 1: 'Poncho', 2: 'Pullover', 3: 'Sweater_Vest'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzH8lYTOEUyq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing import image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
        "\n",
        "pred = []\n",
        "actual = []\n",
        "\n",
        "\n",
        "for attr in os.listdir('/home/eleven/krunal/kk/sweater_style/test'):\n",
        "    for i, file in enumerate(os.listdir(f'/home/eleven/krunal/kk/sweater_style/test/{attr}/')):\n",
        "        test_image = image.load_img(f'/home/eleven/krunal/kk/sweater_style/test/{attr}/'+file, target_size=(299, 299))\n",
        "        test_image = image.img_to_array(test_image)\n",
        "        test_image = np.expand_dims(test_image, axis=0)\n",
        "        test_image = preprocess_input(test_image)\n",
        "        result = model.predict(test_image)[0]\n",
        "        label = label_dict[np.argmax(result)]\n",
        "        pred.append(label)\n",
        "        actual.append(attr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PcPVsuVEUyu",
        "colab_type": "code",
        "colab": {},
        "outputId": "84bb15d4-038e-42cb-cad6-b7fd58d708a3"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "report = classification_report(actual, pred)\n",
        "print(report)\n",
        "\n",
        "matrix = confusion_matrix(actual, pred)\n",
        "print('confusion matrix \\n', matrix)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Cardigan       0.93      0.94      0.94       100\n",
            "      Poncho       1.00      0.93      0.96       100\n",
            "    Pullover       0.89      0.93      0.91       100\n",
            "Sweater_Vest       0.96      0.97      0.97       100\n",
            "\n",
            " avg / total       0.94      0.94      0.94       400\n",
            "\n",
            "confusion matrix \n",
            " [[94  0  4  2]\n",
            " [ 1 93  6  0]\n",
            " [ 5  0 93  2]\n",
            " [ 1  0  2 97]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajsQaEfsEUyy",
        "colab_type": "code",
        "colab": {},
        "outputId": "7a9f7d94-7997-4986-fdf0-8343089ca3ed"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(accuracy_score(actual, pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9425\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcX6qOltEUy2",
        "colab_type": "code",
        "colab": {},
        "outputId": "b4b9a493-71f9-43cb-c6da-8347ba8e4ee7"
      },
      "source": [
        "import pandas as pd\n",
        "def classification_report_csv(report, matrix):\n",
        "    report_data = []\n",
        "    lines = report.split('\\n')\n",
        "    for line in lines[2:-3]:\n",
        "        row = {}\n",
        "        row_data = line.split('      ')\n",
        "        row_data = list(filter(lambda x: len(x), row_data))\n",
        "        row['class'] = row_data[0]\n",
        "        row['precision'] = row_data[1]\n",
        "        row['recall'] = float(row_data[2])\n",
        "        row['f1_score'] = float(row_data[3])\n",
        "        row['support/total'] = float(row_data[4])\n",
        "        report_data.append(row)\n",
        "\n",
        "    df1 = pd.DataFrame(report_data)\n",
        "    df2 = pd.DataFrame(matrix)\n",
        "    df2.columns = list(label_dict.values())\n",
        "    dataframe = pd.concat([df1, df2], axis=1)\n",
        "    kk = dataframe.columns.tolist()\n",
        "    return dataframe[[\"class\"]+kk[5:] + kk[1:5]]\n",
        "\n",
        "df = classification_report_csv(report, matrix)\n",
        "print(df)\n",
        "df.to_csv(\"confusion_matrix.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "          class  Cardigan  Poncho  Pullover  Sweater_Vest  f1_score precision  \\\n",
            "0      Cardigan        94       0         4             2      0.94      0.93   \n",
            "1        Poncho         1      93         6             0      0.96      1.00   \n",
            "2      Pullover         5       0        93             2      0.91      0.89   \n",
            "3  Sweater_Vest         1       0         2            97      0.97      0.96   \n",
            "\n",
            "   recall  support/total  \n",
            "0    0.94          100.0  \n",
            "1    0.93          100.0  \n",
            "2    0.93          100.0  \n",
            "3    0.97          100.0  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "RNo73oGDEUy6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing import image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
        "\n",
        "\n",
        "for folders in glob.glob('/home/eleven/kk/sweater_style/train'):\n",
        "    for k in glob.glob(folders+'/*'):\n",
        "        print(k)\n",
        "        plt.figure()\n",
        "        plt.subplots(280, 3, figsize=(15, 35))\n",
        "        for i, file in enumerate(os.listdir(f'{k}/')):\n",
        "            i += 1\n",
        "            test_image = image.load_img(f'{k}/'+file, target_size=(299, 299))\n",
        "            test_image = image.img_to_array(test_image)\n",
        "            disp = test_image.copy()\n",
        "            test_image = np.expand_dims(test_image, axis=0)\n",
        "            test_image = preprocess_input(test_image)\n",
        "            result = model.predict(test_image)[0]\n",
        "            label = label_dict[np.argmax(result)]\n",
        "#                 print(i)\n",
        "            plt.subplot(24, 3, i)\n",
        "            plt.axis('off')\n",
        "            plt.title(f'Label : {label}', wrap=True, loc='left')\n",
        "            plt.imshow(disp/255.)\n",
        "#         break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiD0ef4mEUzC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# directory = \"validation_knife_set/*\"\n",
        "# for folders in glob.glob('validation_knife_set/*'):\n",
        "#     for k in glob.glob(folders+'/*'):\n",
        "#         print(k)\n",
        "        \n",
        "#     print(folders)\n",
        "#     for k in glob.glob(folders+'/*'):\n",
        "#         print(k)\n",
        "# [k for folders in glob.glob('validation_knife_set/*') for k in glob.glob(folders+'/*')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1VP3wL0EUzG",
        "colab_type": "code",
        "colab": {},
        "outputId": "2a08c03b-dde5-469f-a9a8-c2b764975f02"
      },
      "source": [
        "len([k for k in glob.glob(folders+'/*') for folders in glob.glob(directory)] )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "280"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0w6pXaJZEUzK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# os.listdir('validation_knife_set/steak-knife-set')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqOMqhTtEUzQ",
        "colab_type": "code",
        "colab": {},
        "outputId": "f8bf139b-99a1-4e71-b386-b703d7956297"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import re\n",
        "import pandas as pd\n",
        "import sys\n",
        "import numpy as np\n",
        "import tensorflow.keras\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.optimizers import Nadam\n",
        "\n",
        "\"\"\"\" change the preprocessing, imagesize, dicts:labels, model *.h5 file \"\"\"\n",
        "\n",
        "model = \"final_xception224.h5\"\n",
        "loaded_model = load_model(model)\n",
        "\n",
        "# directory = \"./validations/\"\n",
        "directory = \"validation_knife_set/*\"\n",
        "\n",
        "# dicts = {0: 'Alphabet', 1: 'Animal', 2: 'Argyle', 3: 'Camouflage', 4: 'Cheque', 5: 'Chevron', 6: 'Dot_Print', 7: 'Embroidery', 8: 'Floral', 9: 'Geometric', 10: 'Graphic_Print', 11: 'Houndstooth', 12: 'Paisley', 13: 'Plaid', 14: 'Script', 15: 'Solid', 16: 'Striped', 17: 'Superhero', 18: 'Vehicle'}\n",
        "#dicts = {0: 'Chesterfield', 1: 'Reclining', 2: 'Sofa-bed', 3: 'Standard'}\n",
        "#dicts = {0: 'Arm_chair', 1: 'Parson_chair', 2: 'Side_chair'}\n",
        "\n",
        "# dicts = {0: 'rectangle', 1: 'round', 2: 'square'}\n",
        "dicts = {0:'carving-set', 1:'knife-block-set', 2: 'knife-set-without-block', 3: 'steak-knife-set'}\n",
        "model_prediction = []\n",
        "image = []\n",
        "score = [] \n",
        "# for images in os.listdir(directory):\n",
        "for images in [k for folders in glob.glob('validation_knife_set/*') for k in glob.glob(folders+'/*')]:\n",
        "    image.append(images)\n",
        "    img = tensorflow.keras.preprocessing.image.load_img(images,target_size=(299,299))\n",
        "    img = tensorflow.keras.preprocessing.image.img_to_array(img)\n",
        "    img = np.expand_dims(img,axis=0)\n",
        "    img = tensorflow.keras.applications.inception_v3.preprocess_input(img)\n",
        "    classes = loaded_model.predict(img)\n",
        "    scores = max(classes[0])\n",
        "    score.append(scores)\n",
        "    op = dicts[np.argmax(classes)]\n",
        "    model_prediction.append(op)\n",
        "    data_pd = {\"images\":image,\"predicton\":model_prediction,\"Scores\":score}\n",
        "    #print(data_pd)\n",
        "    frame = pd.DataFrame(data_pd)\n",
        "    frame.to_csv(\"validation.csv\",index=None)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sa83k7xlEUzT",
        "colab_type": "code",
        "colab": {},
        "outputId": "f494cacd-db58-4bc3-9929-cf6ac2c68a6f"
      },
      "source": [
        "import keras"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_bKJjlZEUzX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import numpy\n",
        "# fix random seed for reproducibility\n",
        "numpy.random.seed(7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMrVRJXlEUzb",
        "colab_type": "code",
        "colab": {},
        "outputId": "b517b33a-ce80-42c7-fc30-0f14742aabe7"
      },
      "source": [
        "!pima-indians-diabetes.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/sh: 1: pima-indians-diabetes.csv: not found\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMVCjgWcEUzf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}